name: Performance Benchmark

on:
  schedule:
    # Run every Monday at 9:00 AM UTC
    - cron: '0 9 * * 1'
  workflow_dispatch:  # Allow manual trigger
    inputs:
      compare_baseline:
        description: 'Compare with baseline?'
        required: false
        default: 'true'
        type: boolean

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for baseline comparison

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements-test.txt

      - name: Run performance benchmarks
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Running performance benchmarks..."
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          # Run with benchmark plugin
          python -m pytest tests/unit/ \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-json=benchmark.json \
            --benchmark-columns=min,max,mean,stddev,median,ops \
            --benchmark-histogram=benchmark-histogram \
            -v

          echo "âœ… Benchmarks complete"

      - name: Analyze benchmark results
        run: |
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Analyzing results..."
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          python3 << 'EOF'
          import json
          from pathlib import Path

          benchmark_file = Path('benchmark.json')

          if benchmark_file.exists():
              with open(benchmark_file) as f:
                  data = json.load(f)

              benchmarks = data.get('benchmarks', [])

              print(f"\nğŸ“Š Benchmark Summary:")
              print(f"   Total benchmarks: {len(benchmarks)}")

              # Find slowest tests
              if benchmarks:
                  sorted_benches = sorted(benchmarks, key=lambda x: x['stats']['mean'], reverse=True)

                  print(f"\nâ±ï¸  Top 5 slowest tests:")
                  for bench in sorted_benches[:5]:
                      name = bench['name']
                      mean = bench['stats']['mean']
                      print(f"   {name}: {mean:.4f}s")

                  # Check for tests exceeding 1s
                  slow_tests = [b for b in benchmarks if b['stats']['mean'] > 1.0]
                  if slow_tests:
                      print(f"\nâš ï¸  {len(slow_tests)} tests exceeded 1s:")
                      for bench in slow_tests:
                          print(f"   {bench['name']}: {bench['stats']['mean']:.4f}s")
                  else:
                      print(f"\nâœ… All tests run in < 1s")
          else:
              print("âš ï¸  Benchmark file not found")
          EOF

      - name: Check time budget compliance
        run: |
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Validating time budget compliance..."
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          python3 << 'EOF'
          from pathlib import Path
          import re

          # Define time budgets per mode (in seconds)
          MODE_BUDGETS = {
              'default': 30,
              'debug': 180,
              'optimize': 120,
              'release': 30
          }

          commands_dir = Path('rforge/commands')
          violations = []

          for cmd_file in commands_dir.rglob('*.md'):
              with open(cmd_file, 'r') as f:
                  content = f.read()

              # Extract time budget if present
              match = re.search(r'time_budget:\s*(\d+)', content)
              if match:
                  budget = int(match.group(1))

                  # Extract mode if present
                  mode_match = re.search(r'mode:\s*(\w+)', content)
                  mode = mode_match.group(1) if mode_match else 'default'

                  # Check compliance
                  max_budget = MODE_BUDGETS.get(mode, 30)
                  if budget > max_budget:
                      violations.append({
                          'file': cmd_file.name,
                          'mode': mode,
                          'budget': budget,
                          'max': max_budget
                      })

          if violations:
              print(f"âš ï¸  {len(violations)} time budget violations:")
              for v in violations:
                  print(f"   {v['file']}: {v['budget']}s exceeds {v['mode']} mode max ({v['max']}s)")
          else:
              print("âœ… All time budgets within mode limits")
          EOF

      - name: Generate performance report
        run: |
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Generating performance report..."
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          cat > performance-report.md << 'EOF'
          # Performance Benchmark Report

          **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Commit:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}

          ## Test Execution Time

          Target: All unit tests should complete in < 1 second total.

          ## Mode Time Budgets

          | Mode | Budget | Requirement |
          |------|--------|-------------|
          | Default | 30s | MUST (strict) |
          | Debug | 180s | SHOULD (flexible) |
          | Optimize | 120s | SHOULD (flexible) |
          | Release | 30s | MUST (strict) |

          ## Benchmark Results

          See attached benchmark.json for detailed results.

          ## Recommendations

          - Monitor tests approaching 1s threshold
          - Consider optimizing slow tests
          - Verify mode budgets align with user experience

          ---
          *Generated by GitHub Actions*
          EOF

          echo "âœ… Report generated"

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmark.json
            benchmark-histogram.svg
            performance-report.md
          retention-days: 90

      - name: Upload to benchmark storage (if available)
        if: success()
        continue-on-error: true
        run: |
          # Could integrate with GitHub Pages or external storage
          # For now, just using artifacts
          echo "âœ… Results stored in artifacts"

      - name: Benchmark summary
        if: success()
        run: |
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âœ… Performance benchmarks complete"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "Results saved to artifacts:"
          echo "  benchmark-results-${{ github.sha }}"
          echo ""

  compare:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    needs: benchmark
    if: ${{ github.event.inputs.compare_baseline == 'true' || github.event_name == 'schedule' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements-test.txt

      - name: Download current benchmark
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: ./current

      - name: Get baseline benchmark
        continue-on-error: true
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Fetching baseline benchmark..."
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          # Try to get baseline from main branch artifacts
          # For now, just noting the feature
          echo "âš ï¸  Baseline comparison requires historical data"
          echo "   Will be available after multiple runs"

      - name: Compare performance
        run: |
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Performance comparison..."
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          python3 << 'EOF'
          import json
          from pathlib import Path

          current_file = Path('current/benchmark.json')

          if current_file.exists():
              with open(current_file) as f:
                  data = json.load(f)

              benchmarks = data.get('benchmarks', [])

              print(f"Current run: {len(benchmarks)} benchmarks")
              print("")
              print("Performance trends will be available after multiple runs.")
              print("")

              # Calculate summary stats
              if benchmarks:
                  mean_times = [b['stats']['mean'] for b in benchmarks]
                  avg_mean = sum(mean_times) / len(mean_times)
                  max_mean = max(mean_times)

                  print(f"Summary statistics:")
                  print(f"  Average test time: {avg_mean:.4f}s")
                  print(f"  Slowest test: {max_mean:.4f}s")
                  print(f"  Total tests: {len(benchmarks)}")
          else:
              print("âš ï¸  Current benchmark not found")
          EOF

      - name: Comparison summary
        run: |
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âœ… Comparison complete"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

  report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: [benchmark, compare]
    if: always()

    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: ./results

      - name: Create summary report
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Creating summary report..."
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          cat > summary.md << 'EOF'
          # Weekly Performance Benchmark

          ## Overview

          This report summarizes the performance characteristics of the RForge plugin mode system.

          ## Key Metrics

          - **Test Execution Speed**: All 96 unit tests should complete in < 1s
          - **Time Budget Compliance**: Commands respect mode-specific budgets
          - **Performance Stability**: No significant regressions

          ## Mode System Budgets

          - **default**: 30s (MUST - strict enforcement)
          - **debug**: 180s (SHOULD - flexible guidance)
          - **optimize**: 120s (SHOULD - flexible guidance)
          - **release**: 30s (MUST - strict enforcement)

          ## Recommendations

          1. Monitor tests approaching 1s threshold
          2. Optimize any tests exceeding 500ms
          3. Review mode budgets quarterly
          4. Track performance trends over time

          ## Next Steps

          - Continue weekly benchmarking
          - Build baseline comparison data
          - Set up performance regression alerts

          ---

          **Generated**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Workflow**: ${{ github.workflow }}
          **Run**: ${{ github.run_number }}
          EOF

          cat summary.md

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary
          path: summary.md
          retention-days: 365

      - name: Final summary
        run: |
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ğŸ“Š PERFORMANCE BENCHMARK COMPLETE"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "Results available in artifacts:"
          echo "  - benchmark-results-${{ github.sha }}"
          echo "  - performance-summary"
          echo ""
          echo "Next scheduled run: Next Monday 9:00 AM UTC"
          echo ""
